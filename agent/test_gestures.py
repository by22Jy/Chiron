#!/usr/bin/env python3
"""
æ‰‹åŠ¿æµ‹è¯•å·¥å…·
ç”¨äºæµ‹è¯•å’Œæ˜¾ç¤ºæ‰€æœ‰æ”¯æŒçš„æ‰‹åŠ¿
"""

import cv2
import time
import yaml
import sys
from gestures.mediapipe_detector import MediaPipeGestureDetector

def test_all_gestures():
    print("ğŸ–ï¸  YOLO-LLM æ‰‹åŠ¿è¯†åˆ«æµ‹è¯•å·¥å…·")
    print("=" * 50)
    print("æ˜¾ç¤ºæ‰€æœ‰æ”¯æŒçš„æ‰‹åŠ¿å’Œå®æ—¶è¯†åˆ«ç»“æœ")
    print("æŒ‰ 'q' é”®é€€å‡º")
    print()

    # åˆå§‹åŒ–æ‘„åƒå¤´
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        return

    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    # åˆå§‹åŒ–æ‰‹åŠ¿æ£€æµ‹å™¨
    detector = MediaPipeGestureDetector()

    # æ”¯æŒçš„æ‰‹åŠ¿åˆ—è¡¨
    supported_gestures = {
        'POINT_UP': 'é£ŸæŒ‡æŒ‡å‘ä¸Š',
        'POINT_INDEX': 'é£ŸæŒ‡æŒ‡å‘å‰',
        'THUMBS_UP': 'ğŸ‘ ç‚¹èµ',
        'THUMBS_DOWN': 'ğŸ‘ ç‚¹è¸©',
        'OPEN_PALM': 'âœ‹ å¼ å¼€æ‰‹æŒ',
        'CLOSED_FIST': 'âœŠ æ¡æ‹³',
        'VICTORY': 'âœŒï¸ èƒœåˆ©æ‰‹åŠ¿',
        'OK_SIGN': 'ğŸ‘Œ OKæ‰‹åŠ¿'
    }

    print("æ”¯æŒçš„æ‰‹åŠ¿:")
    for code, name in supported_gestures.items():
        print(f"  {code}: {name}")
    print()

    frame_count = 0
    last_gesture = None
    gesture_count = 0

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1
            frame = cv2.flip(frame, 1)  # æ°´å¹³ç¿»è½¬

            # æ£€æµ‹æ‰‹åŠ¿
            gesture_results = detector.detect_hands(frame)

            current_gesture = None
            if gesture_results:
                result = gesture_results[0]  # å–ç¬¬ä¸€ä¸ªæ£€æµ‹ç»“æœ
                current_gesture = result.gesture_code
                confidence = result.confidence

                # åªæ˜¾ç¤ºæ–°æ‰‹åŠ¿ï¼ˆé¿å…é‡å¤æ˜¾ç¤ºï¼‰
                if current_gesture != last_gesture:
                    gesture_name = supported_gestures.get(current_gesture, current_gesture)
                    print(f"ğŸ¯ æ£€æµ‹åˆ°æ‰‹åŠ¿: {current_gesture} ({gesture_name}) - ç½®ä¿¡åº¦: {confidence:.2f}")
                    last_gesture = current_gesture
                    gesture_count += 1

                # åœ¨ç”»é¢ä¸Šæ˜¾ç¤ºæ‰‹åŠ¿
                cv2.putText(frame, f'Gesture: {current_gesture}',
                          (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.putText(frame, f'Confidence: {confidence:.2f}',
                          (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            else:
                cv2.putText(frame, 'No gesture detected',
                          (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            cv2.putText(frame, f'Frame: {frame_count} | Gestures: {gesture_count}',
                      (10, frame.shape[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            # æ˜¾ç¤ºæ“ä½œæç¤º
            cv2.putText(frame, 'Press SPACE to pause, Q to quit',
                      (10, frame.shape[0] - 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)

            cv2.imshow('Gesture Recognition Test', frame)

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord(' '):
                print("â¸ï¸  æš‚åœ - æŒ‰ç©ºæ ¼ç»§ç»­...")
                cv2.waitKey(0)

    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
    finally:
        cap.release()
        cv2.destroyAllWindows()
        detector.close()

        print(f"\nğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        print(f"   æ€»å¸§æ•°: {frame_count}")
        print(f"   æ£€æµ‹åˆ°æ‰‹åŠ¿: {gesture_count}æ¬¡")
        print(f"   æœ€åæ‰‹åŠ¿: {last_gesture or 'None'}")

if __name__ == "__main__":
    test_all_gestures()